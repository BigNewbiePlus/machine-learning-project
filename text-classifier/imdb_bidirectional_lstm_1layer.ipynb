{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 1500)\n",
      "x_test shape: (25000, 1500)\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 10786s - loss: 0.4413 - acc: 0.7919 - val_loss: 0.4446 - val_acc: 0.8248\n",
      "Epoch 2/20\n",
      "23360/25000 [===========================>..] - ETA: 523s - loss: 0.2362 - acc: 0.9111"
     ]
    }
   ],
   "source": [
    "'''Train a Bidirectional LSTM on the IMDB sentiment classification task.\n",
    "Output after 4 epochs on CPU: ~0.8146\n",
    "Time per epoch on CPU (Core i7): ~150s.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('datasets')\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import codeforces\n",
    "top_words = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "max_len = 1500\n",
    "batch_size = 64\n",
    "embed_dim = 100\n",
    "lstm_size = 100\n",
    "dropout = 0.2\n",
    "epochs = 20\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embed_dim, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(lstm_size)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
