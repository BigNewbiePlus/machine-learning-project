{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "# Attention GRU network\t\t  \n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.add_weight((input_shape[-1],1), initializer=self.init)\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        eij = K.squeeze(eij, axis=-1)\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1, keepdims=True)\n",
    "        \n",
    "        weighted_input = x*K.expand_dims(weights)\n",
    "        return K.sum(weighted_input,axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy\n",
    "import sys\n",
    "sys.path.append('datasets')\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Bidirectional\n",
    "from keras.layers import Embedding, GRU\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "import codeforces\n",
    "\n",
    "numpy.random.seed(7)\n",
    "# set parameters:\n",
    "top_words = 2000\n",
    "max_len = 500\n",
    "batch_size = 128\n",
    "embed_dim = 100\n",
    "filters = 128\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "epochs = 10\n",
    "dropout = 0.2\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=top_words, maxlen=max_len)\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_val), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_val.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(top_words+3, embed_dim, input_length=max_len))\n",
    "model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
    "model.add(AttLayer())\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "print(\"model fitting - attention GRU network\")\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
